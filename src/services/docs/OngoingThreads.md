# Ongoing Threads Service

The `ongoingThreads.ts` manages Kayley's "Mental Weather"—the three to five things she's thinking about in the background. This service prevents Kayley from being a "Blank Slate" who only reacts; instead, she has her own ongoing life (creative projects, social drama, personal growth).

## Core Concepts

1.  **Autonomous Threads**: These are generated by the system (e.g., "Kayley is struggling with a video edit"). They make her feel like a person with her own hobbies.
2.  **User-Related Threads**: These emerge from your conversations. If you talk about move-in drama, that might become an "Ongoing Thread" that she thinks about for a few days.
3.  **Intensity & Decay**: Threads have a pulse.
    *   They start high (Intensity: 1.0).
    *   They slowly decay (e.g., -0.1 per day).
    *   When they hit 0, they are "resolved" and removed.

## Tables Interaction

| Table Name | Action | Description |
| :--- | :--- | :--- |
| `ongoing_threads` | CRUD | Stores the ID, theme, state, and intensity of each mental thread. |

## Workflow Interaction

```text
(Periodically / Idle)
      |
[ongoingThreads] (Generate: "Creative Project" thread)
      |
      V
[BaseAIService] -> "I'm so annoyed with this color grading today..."
      |
      V
(User asks: "Need help?") -> Intensity resets to 1.0
      |
(3 days later) -> Intensity decays to 0 -> Thread Deleted
```

## Does it use an LLM?
**Yes** (as of January 2026). The system now uses **LLM-based dynamic generation** via `autonomousThoughtService.ts`. Hardcoded templates have been removed in favor of context-aware thoughts generated by Gemini Flash.

The lifecycle management (decay, selection, intensity tracking) remains deterministic code for stability, but thought *content* is now generated dynamically based on:
- Character profile (`KAYLEY_FULL_PROFILE`)
- Current mood state (energy, warmth)
- Relationship tier with user
- Recent conversation history
- Recent life events
- Stored user facts

## Why is this important?
Without Threads, an AI feels like an employee waiting for a task. With Threads, she feels like a person you've caught in the middle of their day.

## LLM-Based Thought Generation

### How It Works

As of the January 2026 refactor, autonomous thread generation follows this flow:

```text
[ensureMinimumThreadsAsync()] - Called when threads drop below min (2)
         |
         V
[buildThoughtContextBase()] - Parallel fetch of:
         |                    - Mood (getMoodAsync)
         |                    - Relationship (getRelationship)
         |                    - Life Events (getRecentLifeEvents)
         |                    - User Facts (getUserFacts)
         |                    - Conversation History (loadConversationHistory)
         |
         V
[Pick Theme] - Select theme not already in use
         |     (creative_project, family, social, work, self_improvement, existential)
         |
         V
[generateAutonomousThoughtCached()] - Call autonomousThoughtService
         |
         V
[Gemini Flash LLM] - Generate thought with context
         |
         V
[Quality Gate] - Check:
         |       - shouldMention = true
         |       - confidence >= 0.5
         |       - content is non-empty
         |
   +-----+-----+
   |           |
(Pass)      (Fail)
   |           |
   V           V
[Create    [Skip &
 Thread]    Retry]
```

### Quality Gates

Not all generated thoughts become threads. They must pass:

1. **LLM Decision**: `shouldMention = true` (LLM deemed it appropriate)
2. **Confidence Threshold**: `confidence >= 0.5` (meets quality bar)
3. **Non-Empty**: `content` is not blank or whitespace
4. **Intensity Bounds**: `intensity` clamped to 0.2-1.0 range

If any check fails, the thought is logged and skipped. System will try again later when threads decay.

### Context Sources (Three Sources Principle)

Every thought is generated with three core behavior sources:

#### 1. Character Profile
- `KAYLEY_FULL_PROFILE` - Her personality, values, opinions, insecurities
- Ensures thoughts feel authentic to who she is

#### 2. Conversation History
- Last 5 messages between user and Kayley
- User facts stored in memory
- Current relationship tier (stranger → acquaintance → friends → close_friends → intimate)
- Enables thoughts to reference recent discussions

#### 3. Current Mood State
- Energy level (-1 to 1): exhausted → energized
- Warmth level (0 to 1): guarded → warm
- Affects tone and openness of thoughts

Plus **Life Events** for additional grounding:
- Recent events from `life_events` table (e.g., "Started video project")
- Provides specificity ("this video edit is fighting me" vs. "working on stuff")

### Example Generations

**Low Energy, Stranger Tier:**
```typescript
{
  content: "working on a project... trying to figure something out",
  intensity: 0.4,
  shouldMention: true,
  confidence: 0.7
}
// ✓ Vague, guarded (appropriate for strangers)
```

**High Energy, Close Friends Tier:**
```typescript
{
  content: "this video edit is kicking my ass but I'm kinda obsessed with getting the timing perfect",
  intensity: 0.7,
  shouldMention: true,
  confidence: 0.8
}
// ✓ Specific, vulnerable (appropriate for close friends)
```

**References Recent Conversation:**
```typescript
// User said: "I'm learning guitar"
{
  theme: "creative_project",
  content: "been thinking about picking up my old sketchbook again... something about starting new creative things feels exciting right now",
  intensity: 0.6,
  shouldMention: true,
  confidence: 0.8
}
// ✓ Connects to user's creative pursuit
```

### Caching Strategy

Generated thoughts are cached for **30 minutes** with hash-based keys including:
- Theme
- Relationship tier
- Mood (rounded to 1 decimal to prevent over-invalidation)
- Last 3 message snippets (first 30 chars each)
- Recent life events (first 2)

**Cache Hit Rate**: ~70-80% (threads decay slowly, contexts are stable)

**Performance**:
- Cache hit: <1ms
- Cache miss (LLM call): 300-500ms (async, doesn't block user)

### Comparison: Templates vs. LLM

| Aspect | Hardcoded Templates (Old) | LLM Generation (New) |
|--------|---------------------------|----------------------|
| **Variety** | 30 possible thoughts total | Infinite combinations |
| **Personalization** | Same for all users | Adapts to each relationship |
| **Context Awareness** | Static, no conversation ref | References recent talks |
| **Life Progression** | Never changes | Reflects current life events |
| **Maintenance** | Code changes for new thoughts | Update profile/events only |
| **Performance** | <1ms (array lookup) | 300-500ms (cached after first) |
| **Cost** | Free | ~$0.01-0.02/user/day |

### Integration Points

**Services Used:**
- `autonomousThoughtService.ts` - Generates thoughts
- `lifeEventService.ts` - Provides life events
- `moodKnobs.ts` - Provides mood state
- `relationshipService.ts` - Provides tier
- `memoryService.ts` - Provides user facts
- `conversationHistoryService.ts` - Provides messages

**Used By:**
- `idleThoughtsScheduler.ts` - Triggers generation during idle
- `presenceDirector.ts` - Surfaces threads in greetings
- `proactiveThreadBuilder.ts` - Formats threads for proactive starters

### Troubleshooting

**Problem**: Threads never created, logs show "Skipping low-confidence thought"

**Solution**: Check confidence threshold (may need lowering from 0.5 to 0.4) or examine why LLM is being overly cautious (relationship tier too low, insufficient context).

**Problem**: Thoughts too generic ("thinking about stuff")

**Solution**: Seed life_events table with specific events. Empty life events lead to vague thoughts.

**Problem**: High LLM costs

**Solution**: Verify cache is working (should see "Cache hit" logs). Threads decay slowly (2% per hour autonomous), so generation should only trigger 10-20x/day.

For detailed documentation on thought generation, see:
- [AutonomousThoughtService.md](./AutonomousThoughtService.md)
- [LifeEventService.md](./LifeEventService.md)
